{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "from evaluator import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "path = \"./Auxiliary_DCGAN_\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Found 18009 images...\n"
     ]
    }
   ],
   "source": [
    "image_size = 64\n",
    "\n",
    "trans = transforms.Compose([transforms.ToPILImage(),\n",
    "                            transforms.Resize([image_size, image_size]),\n",
    "                            transforms.CenterCrop([image_size, image_size]),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5)), # normalize to [-1, 1] for the last layer of generator is tanh()\n",
    "                            ])\n",
    "\n",
    "# preprocessing size -> 64x64\n",
    "train = ICLEVRLoader(\"./\", trans=trans, mode=\"train\", preprocessing=None)\n",
    "# test = ICLEVRLoader(\"./\", mode=\"test\")\n",
    "\n",
    "batch_size = 128 # based on paper\n",
    "train_loader = DataLoader(\n",
    "    dataset=train, \n",
    "    batch_size=batch_size,\n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "data = json.load(open(os.path.join('./','test.json')))\n",
    "obj = json.load(open(os.path.join('./','objects.json')))\n",
    "test_labels = data\n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(test_labels[i])):\n",
    "        test_labels[i][j] = obj[test_labels[i][j]]\n",
    "    tmp = np.zeros(len(obj))\n",
    "    tmp[test_labels[i]] = 1\n",
    "    test_labels[i] = tmp\n",
    "test_labels = torch.tensor(test_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_classes, latent_dim, img_shape, n_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.label_emb = nn.Embedding(n_classes, n_classes)\n",
    "        self.input_cnn = nn.Linear(n_classes+latent_dim, 128)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "#             nn.ConvTranspose2d( img_shape, img_shape*8 , 4, 1, 0, bias=False), #input shape [batch_size, 64, 1, 1]\n",
    "            nn.ConvTranspose2d( 128, img_shape*8 , 4, 1, 0, bias=False), # input shape [batch_size, 64, 2, 2]\n",
    "            nn.BatchNorm2d(img_shape*8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (img_shape*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(img_shape * 8, img_shape * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(img_shape * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (img_shape*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( img_shape * 4, img_shape * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(img_shape * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (img_shape*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( img_shape * 2, img_shape, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(img_shape),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (img_shape) x 32 x 32\n",
    "            nn.ConvTranspose2d( img_shape, n_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (n_channels) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((labels, noise), -1).float()\n",
    "        gen_input = self.input_cnn(gen_input).view(-1, 128, 1, 1)\n",
    "        img = self.main(gen_input)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.img_shape= img_shape\n",
    "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.label_linear = nn.Linear(n_classes, img_shape*img_shape)        \n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # input is (n_classes) x 64 x 64\n",
    "            nn.Conv2d(3, img_shape, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (img_shape) x 32 x 32\n",
    "            nn.Conv2d(img_shape, img_shape * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(img_shape * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (img_shape*2) x 16 x 16\n",
    "            nn.Conv2d(img_shape * 2, img_shape * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(img_shape * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (img_shape*4) x 8 x 8\n",
    "            nn.Conv2d(img_shape * 4, img_shape * 8, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(img_shape * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (img_shape*8) x 4 x 4\n",
    "#             nn.Conv2d(img_shape * 8, 1, 4, 1, 0, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Conv2d(img_shape * 8, 1, 4, 1, 0, bias=False), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.aux_layer = nn.Sequential(\n",
    "            nn.Linear(img_shape * 8 * 4 * 4, self.n_classes), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "#         labels = self.label_linear(labels.float()).view(-1, 1, self.img_shape, self.img_shape)\n",
    "#         d_in = torch.cat((img, labels), 1)\n",
    "        out = self.main(img)\n",
    "\n",
    "        validity = self.adv_layer(out).view(-1, 1)\n",
    "        label = self.aux_layer(out.view(out.shape[0], -1))\n",
    "\n",
    "        return validity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "# auxiliary_loss = torch.nn.BCELoss()\n",
    "\n",
    "# model \n",
    "n_classes = 24\n",
    "latent_dim = 100\n",
    "img_shape = 64\n",
    "n_channels = 3\n",
    "load = False\n",
    "if load:\n",
    "    generator = torch.load(\"./Auxiliary_DCGAN_generator.pt\", map_location=device)\n",
    "    discriminator = torch.load(\"./Auxiliary_DCGAN_discriminator.pt\", map_location=device)\n",
    "else:\n",
    "    generator = Generator(n_classes, latent_dim, img_shape, n_channels).to(device)\n",
    "    discriminator = Discriminator(n_classes, img_shape).to(device)\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "\n",
    "# optimizer \n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_acc(filename, g_loss, d_loss, score):\n",
    "    try:\n",
    "        acc = np.load(filename)\n",
    "        np_g_loss = acc['g_loss']\n",
    "        np_d_loss = acc['d_loss']\n",
    "        np_score = acc['score']\n",
    "        np_g_loss = np.append(np_g_loss, g_loss)\n",
    "        np_d_loss = np.append(np_d_loss, d_loss)\n",
    "        np_score = np.append(np_score, score)\n",
    "        np.savez(filename, g_loss=np_g_loss, d_loss=np_d_loss, score=np_score)\n",
    "    except:\n",
    "        g_loss = np.array(g_loss)\n",
    "        d_loss = np.array(d_loss)\n",
    "        score = np.array(score)\n",
    "        np.savez(filename, g_loss=g_loss, d_loss=d_loss, score=score)\n",
    "        \n",
    "def test_eval(generator, epoch):\n",
    "    generator.eval()\n",
    "    eval_model = evaluation_model()\n",
    "    batch_size = test_labels.shape[0]\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))).to(device) # shape [batch_size, latent_dim] with normal distribution\n",
    "    gen_imgs = generator(z, test_labels) # shape [batch_size, 3, 64, 64]\n",
    "    if epoch % 10 == 0:\n",
    "        show_image(gen_imgs)\n",
    "    return eval_model.eval(gen_imgs, test_labels)\n",
    "\n",
    "def show_image(gen_imgs):\n",
    "    # step 1: convert it to [0 ,2]\n",
    "    gen_imgs = gen_imgs +1\n",
    "    \n",
    "    # step 2: convert it to [0 ,1]\n",
    "    gen_imgs = gen_imgs - gen_imgs.min()\n",
    "    gen_imgs = gen_imgs / (gen_imgs.max() - gen_imgs.min())\n",
    "    \n",
    "    grid = make_grid(gen_imgs)\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    plt.imshow(np.transpose(grid.detach().cpu().numpy(), (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db7a34e8dea4869b1af178375e11c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d29f0fec080b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreal_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         d_real_loss = (adversarial_loss(real_pred, valid) + \\\n\u001b[0;32m---> 43\u001b[0;31m                        auxiliary_loss(real_aux, real_labels.float())) / 2\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_aux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "# training \n",
    "epochs = 1\n",
    "n_critic = 1 # number of training steps for discriminator per iter\n",
    "save = False\n",
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "score_list = []\n",
    "max_g_loss = np.inf\n",
    "max_d_loss = np.inf\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "for epoch in tqdm.notebook.tqdm(range(epochs)):\n",
    "    total_d_loss = 0\n",
    "    total_g_loss = 0\n",
    "    generator.train()\n",
    "    \n",
    "    for i, (real_imgs, labels) in enumerate(train_loader):\n",
    "        batch_size = real_imgs.shape[0]\n",
    "\n",
    "        real_imgs = real_imgs[:, :3].to(device)\n",
    "        real_labels = labels.to(device)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))).to(device) # shape [batch_size, latent_dim] with normal distribution\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, real_labels) # shape [batch_size, 3, 64, 64]\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "         # Real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + \\\n",
    "                       auxiliary_loss(real_aux, real_labels.float())) / 2\n",
    "        \n",
    "        print(real_aux[0])\n",
    "                \n",
    "        # Fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, real_labels.float())) / 2\n",
    "\n",
    "         # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            gen_imgs = generator(z, real_labels) # shape [batch_size, 3, 64, 64]\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            fake_validity, pred_label = discriminator(gen_imgs)\n",
    "                        \n",
    "#             g_loss = -torch.mean(fake_validity)\n",
    "            g_loss = 0.5 * (adversarial_loss(fake_validity, valid) + auxiliary_loss(pred_label, real_labels.float()))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        total_d_loss += d_loss.item()\n",
    "        total_g_loss += g_loss.item()\n",
    "    \n",
    "    score = test_eval(generator, epoch+1)\n",
    "    total_d_loss /= len(train_loader)\n",
    "    total_g_loss /= (len(train_loader)/n_critic)\n",
    "    \n",
    "    print(\n",
    "        \"[Epoch %d/%d] [D loss: %f] [G loss: %f] [test score: %f]\"\n",
    "        % (epoch+1, epochs, total_d_loss, total_g_loss, score)\n",
    "    )\n",
    "\n",
    "    # loss save\n",
    "    if save:\n",
    "        save_acc(path+\"loss.npz\", total_g_loss, total_d_loss, score)\n",
    "    else:\n",
    "        g_loss_list.append(total_g_loss)\n",
    "        d_loss_list.append(total_d_loss)\n",
    "    \n",
    "    if save:\n",
    "        if max_g_loss > total_g_loss:\n",
    "            max_g_loss = total_g_loss\n",
    "            torch.save(generator, path+\"generator.pt\")\n",
    "        if max_d_loss > total_d_loss:\n",
    "            max_d_loss = total_d_loss\n",
    "            torch.save(discriminator, path+\"discriminator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "load = True\n",
    "if load:\n",
    "    acc = np.load(path+\"loss.npz\")\n",
    "    g_loss = acc['g_loss']\n",
    "    d_loss = acc['d_loss']\n",
    "    score = acc[\"score\"]\n",
    "\n",
    "plt.title(\"BCELoss\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "# plt.ylabel(\"Score\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "x = np.arange(1, len(g_loss)+1)\n",
    "plt.plot(x, g_loss, label=\"g_loss\")\n",
    "plt.plot(x, d_loss, label=\"d_loss\")\n",
    "# plt.plot(x, score, label=\"test_score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate\n",
    "generator = torch.load(\"./Auxiliary_DCGAN_generator.pt\", map_location=device)\n",
    "# generator.eval()\n",
    "\n",
    "print(test_eval(generator, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = evaluation_model()\n",
    "for images, labels in train_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    out = F.interpolate(images, size=64)  #The resize operation on tensor.\n",
    "    print(out.shape)\n",
    "    print(eval_model.eval(images[:, :3], labels))\n",
    "#     grid = make_grid(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train.__getitem__(0)\n",
    " # step 1: convert it to [0 ,2]\n",
    "img = img +1\n",
    "\n",
    " # step 2: convert it to [0 ,1]\n",
    "img = img - img.min()\n",
    "img = img / (img.max() - img.min())\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(np.transpose(img,(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([3, 3])\n",
    "target = torch.FloatTensor([[1., 1., 0.],\n",
    "                    [1., 0., 0.],\n",
    "                    [0, 1., 1.]])\n",
    "print(x)\n",
    "\n",
    "sig = nn.Sigmoid()\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "x_sig = sig(x)\n",
    "print(x_sig)\n",
    "a = nn.BCELoss()\n",
    "print(a(x_sig, target))\n",
    "\n",
    "x_softmax = softmax(x)\n",
    "print(x_softmax)\n",
    "print(a(x_softmax, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
