{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "from Data import *\n",
    "from train import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.squeeze(pd.read_csv('train.txt', header=None))\n",
    "train_data = split_data(train_data)\n",
    "y_train = y_train_make(len(train_data))\n",
    "\n",
    "train_loader = DataTransformer(train_data, y_train, use_cuda=True)\n",
    "\n",
    "encoder = Encoder(vocab_size=train_loader.vocab_size,\n",
    "                         embedding_size=256,\n",
    "                         output_size=256,\n",
    "                         lat_dim=32).to(device)\n",
    "\n",
    "decoder = Decoder(hidden_size=256,\n",
    "                         output_size=train_loader.vocab_size,\n",
    "                         lat_dim=32,\n",
    "                         max_length=train_loader.max_length,\n",
    "                         teacher_forcing_ratio=1.,\n",
    "                         sos_id=train_loader.SOS_ID,\n",
    "                         use_cuda=True).to(device)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder=encoder,\n",
    "                  decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.squeeze(pd.read_csv('test.txt', header=None))\n",
    "test_data = split_data(test_data)\n",
    "test_data = np.array(test_data)\n",
    "src, trg = src_trg_split(test_data)\n",
    "test_src = []\n",
    "test_trg = []\n",
    "\n",
    "for word in src:\n",
    "    test_src.append(train_loader.vocab.sequence_to_indices(word, add_eos=True))\n",
    "\n",
    "for word in trg:\n",
    "    test_trg.append(train_loader.vocab.sequence_to_indices(word, add_eos=True))\n",
    "\"\"\"\n",
    "sp -> p\n",
    "sp -> pg\n",
    "sp -> tp\n",
    "sp -> tp\n",
    "p  -> tp\n",
    "sp -> pg\n",
    "p  -> sp\n",
    "pg -> sp\n",
    "pg -> p\n",
    "pg -> tp\n",
    "\"\"\"\n",
    "\n",
    "sp = 0\n",
    "tp = 1\n",
    "pg = 2\n",
    "p = 3\n",
    "test_c_src = np.array([sp, sp, sp, sp, p, sp, p, pg, pg, pg]).reshape(-1, 1)\n",
    "test_c_trg = np.array([p, pg, tp, tp, tp, pg, sp, sp, p, tp]).reshape(-1, 1)\n",
    "test_c_src = Variable(torch.LongTensor(to_one_hot(test_c_src))).to(device)\n",
    "test_c_trg = Variable(torch.LongTensor(to_one_hot(test_c_trg))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src_true:      abandon \tTrg_true:   abandoned \tPredict:      abandon \tScore:  0.75148\n",
      "Src_true:         abet \tTrg_true:    abetting \tPredict:      abeting \tScore:  0.68940\n",
      "Src_true:        begin \tTrg_true:      begins \tPredict:       begins \tScore:  1.00000\n",
      "Src_true:       expend \tTrg_true:     expends \tPredict:      expends \tScore:  1.00000\n",
      "Src_true:         sent \tTrg_true:       sends \tPredict:        sents \tScore:  0.28574\n",
      "Src_true:        split \tTrg_true:   splitting \tPredict:     spitting \tScore:  0.67529\n",
      "Src_true:       flared \tTrg_true:       flare \tPredict:       flared \tScore:  0.75984\n",
      "Src_true:  functioning \tTrg_true:    function \tPredict:     function \tScore:  1.00000\n",
      "Src_true:  functioning \tTrg_true:  functioned \tPredict:      fundint \tScore:  0.13162\n",
      "Src_true:      healing \tTrg_true:       heals \tPredict:        heals \tScore:  1.00000\n",
      "Total score: 0.7293375734205133\n"
     ]
    }
   ],
   "source": [
    "seq2seq.load_state_dict(torch.load(\"Lab4_seq2seq_vae_lstm_KL_cond_demo1.pt\", map_location=device))\n",
    "trainer = Trainer(seq2seq, train_loader, y_train, learning_rate=0.001, use_cuda=True)\n",
    "total_score = 0.0\n",
    "for i in range(len(test_src)):\n",
    "    word = train_loader.vocab.indices_to_sequence(test_src[i])\n",
    "    trg_true = train_loader.vocab.indices_to_sequence(test_trg[i])\n",
    "    results = trainer.evaluate(word, test_c_src[i].view(1, -1), test_c_trg[i].view(1, -1))[0]\n",
    "    score = trainer.compute_bleu(results, trg_true)\n",
    "    print(\"Src_true: {:>12}\".format(word), \"\\tTrg_true:{:>12}\".format(trg_true), \"\\tPredict: {:>12}\".format(results), \"\\tScore: {:>8.5f}\".format(score))\n",
    "    total_score += score\n",
    "total_score /= len(test_src)\n",
    "print(\"Total score:\", total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['soften', 'suoveys', 'suoving', 'suoved'], ['servey', 'seffers', 'seffying', 'serveyed'], ['compate', 'compates', 'compeating', 'compeated'], ['remain', 'remains', 'remailing', 'remained'], ['soften', 'softens', 'surving', 'softened'], ['senge', 'sends', 'senting', 'sebled'], ['sunfor', 'sunfors', 'sundering', 'sunfored'], ['comber', 'combers', 'compating', 'combered'], ['compane', 'companes', 'companing', 'siffened'], ['meate', 'meates', 'meating', 'meated'], ['stack', 'suvers', 'suvering', 'suvered'], ['siffen', 'suvides', 'suviding', 'suvided'], ['compate', 'compates', 'compating', 'compated'], ['suver', 'suvers', 'suvering', 'suver'], ['meare', 'meares', 'meating', 'meased'], ['comber', 'combers', 'compating', 'combered'], ['compress', 'compresses', 'compressing', 'compressed'], ['compete', 'competes', 'comperting', 'compented'], ['subvise', 'subles', 'subles', 'subled'], ['meature', 'meacks', 'meating', 'meatured'], ['compate', 'compates', 'compating', 'compated'], ['compete', 'competes', 'competing', 'competed'], ['disunate', 'disunites', 'disuniting', 'disunated'], ['suffer', 'suffers', 'suffering', 'suffered'], ['compete', 'compents', 'compenting', 'compened'], ['outsel', 'outsels', 'outsending', 'outseld'], ['meat', 'meats', 'meating', 'meated'], ['compate', 'compates', 'compening', 'compated'], ['compete', 'competes', 'competing', 'competed'], ['remain', 'remains', 'resulting', 'remained'], ['sefle', 'sefles', 'sefling', 'sefled'], ['disfeate', 'disfesses', 'disfeiging', 'disfessed'], ['dispend', 'disgusses', 'dispenting', 'displesed'], ['suffer', 'competes', 'compating', 'compened'], ['mover', 'movers', 'movering', 'movered'], ['compate', 'compates', 'competing', 'competed'], ['soften', 'softens', 'softening', 'softened'], ['suver', 'suvers', 'suvering', 'suvered'], ['compated', 'compates', 'compating', 'compated'], ['suvey', 'suvels', 'suveling', 'suvered'], ['soften', 'softens', 'softening', 'softened'], ['siffene', 'surveys', 'surviving', 'siffened'], ['soften', 'softens', 'softening', 'softened'], ['compressed', 'compens', 'compening', 'compened'], ['soften', 'softs', 'softening', 'softened'], ['mover', 'movers', 'meating', 'movered'], ['mover', 'movers', 'movering', 'movered'], ['stand', 'stapes', 'stacking', 'stained'], ['compate', 'compates', 'compening', 'compated'], ['over', 'overs', 'overting', 'overted'], ['stiffen', 'stiffens', 'stiffening', 'survited'], ['survey', 'surveys', 'surveying', 'surveyed'], ['remark', 'remains', 'remarking', 'remarked'], ['mover', 'movers', 'movering', 'movered'], ['specame', 'serveys', 'specating', 'suffered'], ['bestow', 'bestows', 'bestowing', 'bestowed'], ['survey', 'surveys', 'surveying', 'surveyed'], ['result', 'results', 'resulting', 'resulted'], ['disate', 'disates', 'disating', 'disated'], ['suvel', 'suvels', 'suvelling', 'suvell'], ['remain', 'remains', 'remaining', 'remained'], ['measure', 'measures', 'measuring', 'measured'], ['remake', 'remakes', 'remaking', 'remaked'], ['compate', 'compates', 'competing', 'competed'], ['avort', 'avotes', 'avoting', 'avoted'], ['compete', 'competes', 'competing', 'competed'], ['suver', 'suvers', 'suvering', 'suvered'], ['remain', 'remains', 'remaining', 'remained'], ['survey', 'measures', 'measuring', 'measured'], ['soften', 'sermines', 'serming', 'softened'], ['stiffen', 'competes', 'stiffening', 'stiffened'], ['compate', 'compates', 'competing', 'compated'], ['subvis', 'subvises', 'subving', 'subvised'], ['repail', 'oppeates', 'repaiding', 'oppeated'], ['result', 'results', 'resulting', 'resulted'], ['suver', 'suvels', 'suvering', 'suvered'], ['resume', 'resumings', 'resuming', 'resumed'], ['compate', 'compates', 'compesing', 'compened'], ['compate', 'compates', 'competing', 'compated'], ['demand', 'demands', 'deaming', 'demanded'], ['compete', 'competes', 'suvering', 'suverled'], ['outsell', 'outsends', 'outselling', 'overtake'], ['compate', 'compates', 'compating', 'compated'], ['sundie', 'suntifies', 'suvving', 'suvvey'], ['competed', 'competed', 'compeating', 'compened'], ['suoge', 'suoges', 'suoging', 'suoked'], ['remain', 'remained', 'remaining', 'remained'], ['suffer', 'suffers', 'suffering', 'suffered'], ['number', 'numbers', 'numbering', 'numbered'], ['compene', 'competes', 'compening', 'compened'], ['compete', 'competes', 'competing', 'competed'], ['disteer', 'distesses', 'distering', 'distered'], ['compate', 'competes', 'competing', 'competed'], ['suffer', 'suffers', 'suffering', 'suffered'], ['distance', 'distances', 'distancing', 'distanced'], ['suffers', 'paggests', 'paggeting', 'paggeted'], ['compare', 'compares', 'comparing', 'compated'], ['compate', 'compates', 'compating', 'compated'], ['over', 'overs', 'overting', 'overted'], ['compete', 'competes', 'competing', 'competed']]\n",
      "Total Gaussian score: 0.21\n"
     ]
    }
   ],
   "source": [
    "seq2seq.load_state_dict(torch.load(\"Lab4_seq2seq_vae_lstm_KL_cond_demo2.pt\", map_location=device))\n",
    "trainer = Trainer(seq2seq, train_loader, y_train, learning_rate=0.001, use_cuda=True)\n",
    "def reparaterization_trick(mean, logv):\n",
    "        std = torch.exp(0.5*logv)\n",
    "        eps = torch.randn_like(std)\n",
    "        return  mean + eps * std\n",
    "    \n",
    "def Gaussian_score(words):\n",
    "    words_list = []\n",
    "    score = 0\n",
    "    yourpath = './train.txt'#should be your directory of train.txt\n",
    "    with open(yourpath,'r') as fp:\n",
    "        for line in fp:\n",
    "            word = line.split(' ')\n",
    "            word[3] = word[3].strip('\\n')\n",
    "            words_list.extend([word])\n",
    "        for t in words:\n",
    "            for i in words_list:\n",
    "                if t == i:\n",
    "                    score += 1\n",
    "    return score/len(words)\n",
    "\n",
    "    \n",
    "label = torch.LongTensor([[1, 0, 0, 0],\n",
    "                       [0, 1, 0, 0],\n",
    "                       [0, 0, 1, 0],\n",
    "                       [0, 0, 0, 1]]).to(device)\n",
    "\n",
    "words = []\n",
    "for i in range(100):\n",
    "    hidden_mean = torch.randn([1, 1, 32]).to(device)\n",
    "    hidden_logv = torch.randn([1, 1, 32]).to(device)\n",
    "    cell_mean = torch.randn([1, 1, 32]).to(device)\n",
    "    cell_logv = torch.randn([1, 1, 32]).to(device)\n",
    "    \n",
    "    encoder_hidden = reparaterization_trick(hidden_mean, hidden_logv)\n",
    "    encoder_hidden = decoder.latent2hidden(encoder_hidden)\n",
    "    encoder_cell = reparaterization_trick(cell_mean, cell_logv)\n",
    "    encoder_cell = decoder.latent2hidden(encoder_cell)\n",
    "    \n",
    "    tmp = []\n",
    "    for i in range(4):\n",
    "        hidden = torch.cat([encoder_hidden, label[i].view(1, 1, 4)], dim=2)\n",
    "        cell = torch.cat([encoder_cell, label[i].view(1, 1, 4)], dim=2)\n",
    "        decoded_indices = decoder.evaluate(context_vector=hidden, decoder_cell=cell)\n",
    "        results = []\n",
    "        for indices in decoded_indices:\n",
    "            results.append(train_loader.vocab.indices_to_sequence(indices))\n",
    "        tmp.append(results[0])\n",
    "    words.append(tmp)\n",
    "print(words)\n",
    "print(\"Total Gaussian score:\", Gaussian_score(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
